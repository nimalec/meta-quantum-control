# Meta-RL for Quantum Control - Experiment Configuration

# Random seed
seed: 42

# Quantum System
psd_model: 'one_over_f'  # 'one_over_f', 'lorentzian', 'double_exp'
horizon: 1.0  # Total evolution time (arbitrary units)
target_gate: 'hadamard'  # 'hadamard', 'pauli_x', 'pauli_y'

# Task Distribution
task_dist_type: 'uniform'
alpha_range: [0.5, 2.0]      # Spectral exponent range
A_range: [0.005, 0.03]       # FIXED: Further reduced for stable initial training (was [0.02, 0.15])
omega_c_range: [2.0, 8.0]    # Cutoff frequency range

# Policy Network
task_feature_dim: 3
hidden_dim: 128
n_hidden_layers: 2
n_segments: 20
n_controls: 2
output_scale: 2.0  # FIXED: Increased from 1.0 to allow stronger control signals for better gate fidelity
activation: 'tanh'

# MAML Hyperparameters
inner_lr: 0.001  # FIXED: Further reduced from 0.005 for stable gradients with differentiable simulator
inner_steps: 5   # FIXED: Increased from 3 to allow sufficient adaptation (meta-learning needs this!)
meta_lr: 0.001
first_order: true  # FIXED: Use FOMAML initially to avoid complex eigenvalue gradient issues

# Training
n_iterations: 2000
tasks_per_batch: 8  # FIXED: Increased from 4 for more stable meta-gradient estimates
n_support: 10
n_query: 10
log_interval: 10
val_interval: 50
val_tasks: 20

# Checkpointing
save_dir: 'checkpoints'

# Robust Baseline (for comparison)
robust_type: 'minimax'  # 'average', 'minimax', 'cvar'
robust_iterations: 2000
robust_tasks_per_batch: 16

# Optimality Gap Experiments
gap_n_samples: 100
gap_K_values: [1, 3, 5, 10, 20]
gap_variance_range: [0.01, 0.05, 0.1, 0.2, 0.5]