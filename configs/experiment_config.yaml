# Meta-RL for Quantum Control - Experiment Configuration

# Quantum System
psd_model: 'one_over_f'  # 'one_over_f', 'lorentzian', 'double_exp'
horizon: 1.0  # Total evolution time (arbitrary units)
target_gate: 'pauli_x'  # 'hadamard', 'pauli_x', 'pauli_y'
noise_type: 'frequency'

# Task Distribution
#(0.1, 4.0), "A": (100, 1e5), "omega_c": (0, 800) 
task_dist_type: 'uniform'
alpha_range: [0.1, 4.0]      # Spectral exponent range
A_range: [1, 100000]         # FIXED: Increased to [0.01, 0.1] for better task diversity and meta-learning
omega_c_range: [1, 100]    # Cutoff frequency range
num_qubits: 1

# Policy Network
task_feature_dim: 3
hidden_dim: 256
n_hidden_layers: 2
n_segments: 100
n_controls: 2
output_scale: 2.0  # FIXED: Increased from 1.0 to allow stronger control signals for better gate fidelity
activation: 'tanh'

# MAML Hyperparameters
inner_lr: 0.01  # FIXED: Further reduced from 0.005 for stable gradients with differentiable simulator
inner_steps: 5   # FIXED: Increased from 3 to allow sufficient adaptation (meta-learning needs this!)
meta_lr: 0.001
first_order: false # FIXED: Use FOMAML initially to avoid complex eigenvalue gradient issues

# Training
n_iterations: 55
tasks_per_batch: 32  # FIXED: Increased from 4 for more stable meta-gradient estimates
n_support: 10
n_query: 10
log_interval: 2
val_interval: 2
val_tasks: 32

#New parameters 
drift_strength: 0.5 
sequence: 'ramsey'
model_types: ['one_over_f']
model_probs: [1.0]
dt_training: 0.01 
use_rk4_training: True
omega0: 6.28318530718 
Gamma_h: 100 
 

# Checkpointing
save_dir: 'checkpoints'
