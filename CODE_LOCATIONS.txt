================================================================================
EXACT CODE LOCATIONS FOR ROBUST EXPERIMENTS & OPTIMIZATION METHODS
================================================================================

ROBUST EXPERIMENT FILES
================================================================================

PRIMARY TRAINING SCRIPTS:
├─ /experiments/train_meta.py
│  └─ Main MAML meta-learning training
│     Key classes: MAML, MAMLTrainer, PulsePolicy
│     Helper functions imported from: create_quantum_system, create_task_distribution
│
├─ /experiments/train_robust.py
│  └─ Robust baseline training (no adaptation)
│     Key classes: RobustPolicy, RobustTrainer
│     Robust types: average, minimax, cvar
│     Lines 100-107: Robust type selection
│
└─ /experiments/eval_gap.py
   └─ Gap evaluation utilities
      Key class: OptimalityGapComputer

PAPER RESULTS PIPELINE:
├─ /experiments/paper_results/generate_all_results.py
│  └─ MASTER orchestration script
│     Lines 115-263: Main execution flow
│     Orchestrates: gap_vs_k, gap_vs_variance, constants_validation
│     Generates: comparison table, validation checks
│
├─ /experiments/paper_results/experiment_gap_vs_k.py
│  └─ Figure 1: Gap vs Adaptation Steps K
│     Lines 41-48: Function signature
│     Lines 36-38: Exponential model fit
│     K values: [1, 2, 3, 5, 7, 10, 15, 20]
│     Target: R² ≈ 0.96
│
├─ /experiments/paper_results/experiment_gap_vs_variance.py
│  └─ Figure 2: Gap vs Task Variance σ²_S
│     Lines 85-93: Function signature
│     Lines 35-37: Linear model fit
│     Fixed K: 5
│     Target: R² ≈ 0.94
│
├─ /experiments/paper_results/experiment_constants_validation.py
│  └─ Table 1: Physics Constants Validation
│     Estimates: C_sep, μ, L, L_F, c_quantum
│     Validation ratio bounds: 0.5-2.0×
│
├─ /experiments/paper_results/experiment_system_scaling.py
│  └─ System size scaling experiments
│     Tests: 1 qubit, 2 qubits
│
├─ /experiments/paper_results/plot_fidelity_vs_k.py
│  └─ Advanced fidelity vs K plotting
│     Includes confidence intervals
│
└─ /experiments/paper_results/plot_validation_metrics.py
   └─ Validation metric visualization

================================================================================
OPTIMIZATION METHOD IMPLEMENTATIONS
================================================================================

FILE: /metaqctrl/baselines/robust_control.py

SECTION 1: RobustPolicy Class
├─ Lines 26-221: RobustPolicy implementation
├─ Lines 26-53: __init__ method
├─ Lines 55-88: train_step method
├─ Lines 90-113: _average_robust_loss method
│  Formula: Loss = E[L(π, θ)]
│  Simple mean of losses
│
├─ Lines 115-144: _minimax_robust_loss method
│  Formula: Loss = max_θ L(π, θ) via LogSumExp
│  Line 135: β = 10.0 (temperature parameter)
│  LogSumExp approximation: (1/β) * log(Σ exp(β*x))
│
├─ Lines 146-176: _cvar_robust_loss method
│  Formula: Loss = E[worst α% of losses]
│  Line 150: α = 0.1 (default)
│  Line 165: k = max(1, int(alpha * len(losses)))
│
├─ Lines 178-201: evaluate method
│  Validation evaluation on test tasks
│
└─ Lines 203-220: save/load methods

SECTION 2: RobustTrainer Class
├─ Lines 223-323: RobustTrainer implementation
├─ Lines 226-240: __init__ method
├─ Lines 242-323: train method
│  Line 260: Prints robust policy type
│  Lines 265-290: Main training loop
│  Lines 292-317: Validation & early stopping
│  Line 316: Save best model
│  Line 321: Save final model

SECTION 3: H2RobustControl Class
├─ Lines 326-385: Classical H∞ / H2 optimal control baseline
├─ Lines 334-348: __init__ with system matrices
├─ Lines 350-369: solve_h_infinity method
├─ Lines 371-385: _solve_lqr fallback
│  Uses scipy.linalg.solve_continuous_are
│  Algebraic Riccati equation solver

SECTION 4: DomainRandomization Class
├─ Lines 388-432: Domain randomization baseline
├─ Lines 396-404: __init__ with randomization strength
│  Line 399: randomization_strength = 0.1 (10%)
├─ Lines 406-412: randomize_task method
│  Adds multiplicative Gaussian noise
└─ Lines 414-432: train_step with augmentation

SECTION 5: GRAPEOptimizer Class
├─ Lines 435-735: GRAPE baseline implementation
│
├─ Lines 447-488: __init__ method
│  Line 452: n_segments (pulse discretization)
│  Line 453: n_controls (X, Y channels)
│  Line 454: T = 1.0 (evolution time)
│  Line 455: control_bounds = (-5.0, 5.0)
│  Line 456: learning_rate = 0.1
│  Line 457: method = 'adam' (options: adam, lbfgs, gradient)
│  Lines 477-479: Initialize control parameters as torch.nn.Parameter
│  Lines 482-487: Setup optimizer (Adam/LBFGS/SGD)
│
├─ Lines 492-586: optimize method (single-task)
│  Args: simulate_fn, task_params, max_iterations, tolerance, verbose
│  Line 520-547: closure function for optimization
│  Lines 525: Bound application via tanh
│  Lines 531: Call simulator
│  Lines 534: Loss = -fidelity (negated for maximization)
│  Lines 538-540: Finite difference gradient computation
│  Lines 549-555: LBFGS or standard step
│  Lines 558-563: Track fidelity history & gradients
│  Returns: optimal_controls (numpy array)
│
├─ Lines 588-593: _apply_bounds method
│  Tanh-based amplitude bounding
│  Formula: scale * tanh(controls) + offset
│
├─ Lines 595-633: _compute_finite_difference_gradient method
│  Lines 607-631: For each control element:
│  Perturb by epsilon, compute fidelity difference
│  Line 631: gradient[i,j] = -(f_plus - f0) / epsilon
│  Computational cost: O(n_segments × n_controls) evaluations
│
├─ Lines 635-718: optimize_robust method (multi-task)
│  Args: simulate_fn, task_distribution, max_iterations, robust_type, verbose
│  Line 657: Print optimization info
│  Lines 659-707: Main loop over iterations
│  Lines 663-669: Evaluate on all tasks
│  Lines 672-677: Aggregate objective (average or worst-case)
│  Lines 680-692: Compute gradients per task
│  Line 695-697: Set aggregate gradient
│  Line 700: Optimization step
│  Lines 712-716: Final validation & print results
│  Returns: robust_controls (numpy array)
│
├─ Lines 720-725: reset method
│  Reinitialize controls to random values
│
└─ Lines 727-735: get/set_controls helper methods

================================================================================
CORE COMPONENTS (NON-BASELINE)
================================================================================

FILE: /metaqctrl/meta_rl/maml.py
├─ Lines 24-76: MAML class __init__
│  Line 40: inner_lr = 0.01 (adaptation learning rate)
│  Line 41: inner_steps = 5 (K, default adaptation steps)
│  Line 42: meta_lr = 0.001 (meta-learning rate)
│  Line 43: first_order = False (enable second-order by default)
│
├─ Lines 78-124: inner_loop method (first-order MAML)
│  Task adaptation via K gradient steps
│  Lines 101: deepcopy policy for task isolation
│  Lines 111-122: K-step loop
│  Returns: adapted_policy, losses
│
└─ Lines 126-150: inner_loop_higher method (second-order MAML)
   Uses `higher` library for differentiable optimization

FILE: /metaqctrl/meta_rl/policy.py
├─ Lines 13-137: PulsePolicy class
│  Line 24: task_feature_dim = 3 (α, A, ωc)
│  Line 25: hidden_dim = 128
│  Line 26: n_hidden_layers = 2
│  Line 27: n_segments = 20
│  Line 28: n_controls = 2
│  Lines 91-117: forward method
│  Returns: (batch, n_segments, n_controls) control tensor
│
└─ Lines 140-: TaskFeatureEncoder (optional)
   Learn task representations from noise parameters

FILE: /metaqctrl/theory/optimality_gap.py
├─ Lines 18-32: GapConstants dataclass
│  Fields: C_sep, mu, L, L_F, C_K
│  Line 26-32: gap_lower_bound method
│
├─ Lines 35-196: OptimalityGapComputer class
├─ Lines 56-116: compute_gap method
│  Formula: Gap = E[F_meta(adapted)] - E[F_robust]
│  Lines 88-92: Adapt meta-policy (K steps)
│  Lines 95-98: Evaluate on tasks
│  Returns: gap metrics dictionary
│
└─ Lines 118-162: _adapt_policy method
   Performs K-step gradient adaptation

FILE: /metaqctrl/theory/quantum_environment.py
├─ Lines 18-67: QuantumEnvironment class __init__
│  Unified interface for simulation
│  Line 58-61: Cache for Lindblad operators & simulators
│
├─ Lines 104-120: get_lindblad_operators method
│  Caching for efficiency
│
├─ Lines 122-146: get_simulator method
│  Returns: LindbladSimulator instance
│
└─ Lines 148-: evaluate_controls method (main evaluation API)

FILE: /metaqctrl/quantum/lindblad.py
├─ LindbladSimulator class
│  Simulates open quantum systems via Lindblad master equation
│  ρ_dot = -i[H, ρ] + Σ_k L_k ρ L_k† - 1/2{L_k† L_k, ρ}
│
└─ evolve method
   Integrates master equation for control sequences

FILE: /metaqctrl/quantum/lindblad_torch.py
├─ DifferentiableLindbladSimulator class
│  PyTorch implementation for gradient computation
│
└─ Enables backpropagation through quantum simulation

FILE: /metaqctrl/quantum/noise_models.py
├─ NoiseParameters class
│  Line: alpha (spectral exponent)
│  Line: A (PSD amplitude)
│  Line: omega_c (cutoff frequency)
│
├─ TaskDistribution class
│  Samples random task parameters
│  Methods: sample, compute_variance
│
├─ NoisePSDModel class
│  Model types: one_over_f, lorentzian, double_exp
│
└─ PSDToLindblad class
   Converts PSD to Lindblad operators

FILE: /metaqctrl/quantum/gates.py
├─ TargetGates class
│  hadamard(), pauli_x(), pauli_y(), pauli_z()
│  cnot(), rx(θ), etc.
│
└─ state_fidelity function
   Computes ⟨ψ|ρ|ψ⟩ or tr(U†V) depending on input

================================================================================
CONFIGURATION FILES
================================================================================

/configs/experiment_config.yaml
├─ Lines 1-55: Master configuration
├─ Lines 6-16: Quantum system setup
├─ Lines 12-16: Task distribution ranges
├─ Lines 19-26: Policy network architecture
├─ Lines 28-32: MAML hyperparameters
├─ Lines 34-41: Training configuration
├─ Lines 43-49: Robust baseline configuration
└─ Lines 51-54: Gap experiment configuration

/configs/test_config.yaml
└─ Minimal config for testing

================================================================================
EXAMPLE FILES
================================================================================

/examples/grape_example.py
├─ Lines 42-45: Quantum system setup
├─ Lines 68-88: Simulator function definition
│  Key: Takes controls (numpy), task_params
│  Returns: fidelity (float)
│
├─ Lines 92-104: GRAPEOptimizer initialization
│  n_segments=20, n_controls=2
│  learning_rate=0.1, method='adam'
│
├─ Lines 111-117: Single-task optimization
│  50 iterations maximum
│
├─ Lines 130-168: Plotting results
│  Convergence, optimal pulses, gradient norms
│
└─ Lines 179-197: Robust GRAPE (multi-task)
   optimize_robust over task distribution

================================================================================
KEY IMPORTS & DEPENDENCIES
================================================================================

torch
├─ torch.nn.Module (base class for policies)
├─ torch.optim (Adam, SGD, LBFGS optimizers)
└─ torch.autograd (gradient computation)

numpy
├─ Vectorized computations
├─ Random number generation (np.random.default_rng)
└─ Linear algebra (np.linalg)

scipy
├─ scipy.linalg.solve_continuous_are (Riccati solver)
├─ scipy.optimize.minimize (general optimization)
└─ scipy.integrate.odeint (ODE integration)

Optional:
├─ higher (second-order MAML)
├─ cvxpy (convex optimization for H∞)
└─ matplotlib, seaborn (visualization)

================================================================================
USAGE FLOW: HOW TO RUN EXPERIMENTS
================================================================================

1. CONFIGURE EXPERIMENT
   Edit: /configs/experiment_config.yaml
   Key parameters:
   - alpha_range, A_range, omega_c_range (task distribution)
   - inner_lr, inner_steps (MAML adaptation)
   - robust_type (minimax, average, cvar)
   - n_iterations, tasks_per_batch (training)

2. TRAIN META-LEARNER
   Command: python experiments/train_meta.py --config configs/experiment_config.yaml
   Output: checkpoints/maml_best.pt
   
3. TRAIN ROBUST BASELINE
   Command: python experiments/train_robust.py --config configs/experiment_config.yaml
   Output: checkpoints/robust_best.pt
   
4. GENERATE PAPER RESULTS
   Command: python experiments/paper_results/generate_all_results.py \
            --meta_path checkpoints/maml_best.pt \
            --robust_path checkpoints/robust_best.pt \
            --output_dir results/paper
   Output: 
   - results/paper/gap_vs_k/figure.pdf
   - results/paper/gap_vs_variance/figure.pdf
   - results/paper/constants_validation/constants_visualization.pdf
   - results/paper/summary_table.txt

5. (OPTIONAL) RUN GRAPE BASELINE
   Command: python examples/grape_example.py
   Output: grape_optimization_results.png
   
   To integrate with paper results:
   - Create: experiments/paper_results/experiment_grape_baseline.py
   - Call from: generate_all_results.py main()

================================================================================
MODIFICATION POINTS FOR CUSTOM EXPERIMENTS
================================================================================

To Change Robust Type:
  File: /configs/experiment_config.yaml
  Line: robust_type: 'minimax'
  Options: 'average', 'minimax', 'cvar'
  Or: Edit /metaqctrl/baselines/robust_control.py line 100-107

To Add New Optimization Method:
  File: /metaqctrl/baselines/robust_control.py
  Location: Add new class after line 432 (before GRAPEOptimizer)
  Template: Implement class with train_step(batch, loss_fn) method

To Change Task Distribution:
  File: /configs/experiment_config.yaml
  Lines: alpha_range, A_range, omega_c_range
  Or: Edit /metaqctrl/quantum/noise_models.py TaskDistribution

To Modify MAML:
  File: /metaqctrl/meta_rl/maml.py
  Edit: inner_lr, inner_steps, meta_lr parameters
  Or: Modify inner_loop/inner_loop_higher methods

To Change Network Architecture:
  File: /metaqctrl/meta_rl/policy.py
  Edit: PulsePolicy.__init__ hidden_dim, n_hidden_layers

================================================================================
END CODE LOCATIONS GUIDE
================================================================================
