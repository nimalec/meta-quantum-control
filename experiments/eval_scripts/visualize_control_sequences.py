"""
Validation Script 4: Time-Domain Control Sequence Visualization

This script visualizes the actual control pulse sequences generated by
the meta-learned policy under different adaptation scenarios:

1. Robust baseline (policy trained on average task, K=0)
2. Detuning-adapted (high/low omega_c, K=5)
3. T1-adapted (high/low noise amplitude A, K=5)
4. Pulse evolution for single task over K = [0, 1, 3, 5]

This provides intuition for how the policy adapts its control strategy
to different noise conditions.

Expected output: Multi-panel plots showing control amplitudes vs time
"""

import os
import sys
import numpy as np
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from typing import Dict, List, Tuple
from typer import Typer
import copy

from metaqctrl.quantum.noise_models import TaskDistribution, NoiseParameters
from metaqctrl.meta_rl.policy import PulsePolicy
from metaqctrl.theory.quantum_environment import create_quantum_environment, get_target_state_from_config
from metaqctrl.utils.checkpoint_utils import load_policy_from_checkpoint

app = Typer()


def get_control_sequence(
    policy: torch.nn.Module,
    task: NoiseParameters,
    config: Dict,
    env,
    adapt: bool = False,
    K: int = 0,
    inner_lr: float = 0.01
) -> Tuple[np.ndarray, float]:
    """
    Get control sequence from policy, optionally with adaptation.

    Args:
        policy: Policy network
        task: NoiseParameters
        config: Configuration dict
        env: QuantumEnvironment
        adapt: If True, adapt policy before generating controls
        K: Number of adaptation steps (if adapt=True)
        inner_lr: Inner loop learning rate

    Returns:
        controls: Control sequence array (n_segments, n_controls)
        fidelity: Achieved gate fidelity
    """
    task_features = torch.tensor(
        [task.alpha, task.A, task.omega_c],
        dtype=torch.float32
    )

    if adapt and K > 0:
        # Adapt policy
        adapted_policy = copy.deepcopy(policy)
        adapted_policy.train()

        for k in range(K):
            loss = env.compute_loss_differentiable(
                adapted_policy, task, device=torch.device('cpu')
            )
            loss.backward()
            with torch.no_grad():
                for param in adapted_policy.parameters():
                    if param.grad is not None:
                        param -= inner_lr * param.grad
                        param.grad.zero_()

        policy_to_use = adapted_policy
    else:
        policy_to_use = policy

    # Generate controls
    policy_to_use.eval()
    with torch.no_grad():
        controls = policy_to_use(task_features).detach().numpy()

    # Evaluate fidelity
    fidelity = env.compute_fidelity(controls, task)

    return controls, fidelity


def visualize_scenario_comparison(
    meta_policy_path: str,
    robust_policy_path: str,
    config: Dict,
    output_dir: str = "results/control_sequences"
):
    """
    Scenario 1-3: Compare control sequences for different adaptation scenarios.

    Generates plots showing:
    (a) Robust baseline (no adaptation)
    (b) Detuning-adapted (high omega_c vs low omega_c)
    (c) T1-adapted (high A vs low A)

    Each panel shows the two control channels (X and Y) over time.
    """
    print("=" * 80)
    print("CONTROL SEQUENCE VISUALIZATION: Scenario Comparison")
    print("=" * 80)

    os.makedirs(output_dir, exist_ok=True)

    # Load policies
    print("\n[1/4] Loading policies...")
    meta_policy = load_policy_from_checkpoint(
        meta_policy_path, config, eval_mode=False, verbose=True
    )
    robust_policy = load_policy_from_checkpoint(
        robust_policy_path, config, eval_mode=True, verbose=True
    )

    # Create environment
    print("\n[2/4] Creating environment...")
    target_state = get_target_state_from_config(config)
    env = create_quantum_environment(config, target_state)

    # Define baseline (average) task
    baseline_task = NoiseParameters(
        alpha=np.mean(config.get('alpha_range', [0.5, 2.0])),
        A=np.mean(config.get('A_range', [0.05, 0.3])),
        omega_c=np.mean(config.get('omega_c_range', [2.0, 8.0]))
    )

    # Define extreme tasks for adaptation
    # High/low detuning (omega_c)
    omega_c_range = config.get('omega_c_range', [2.0, 8.0])
    high_detuning_task = NoiseParameters(
        alpha=baseline_task.alpha,
        A=baseline_task.A,
        omega_c=omega_c_range[1]  # High cutoff frequency
    )
    low_detuning_task = NoiseParameters(
        alpha=baseline_task.alpha,
        A=baseline_task.A,
        omega_c=omega_c_range[0]  # Low cutoff frequency
    )

    # High/low T1 (via amplitude A - higher A means more decoherence, lower T1)
    A_range = config.get('A_range', [0.05, 0.3])
    high_noise_task = NoiseParameters(
        alpha=baseline_task.alpha,
        A=A_range[1],  # High noise amplitude (low T1)
        omega_c=baseline_task.omega_c
    )
    low_noise_task = NoiseParameters(
        alpha=baseline_task.alpha,
        A=A_range[0],  # Low noise amplitude (high T1)
        omega_c=baseline_task.omega_c
    )

    K_adapt = 5
    inner_lr = config.get('inner_lr', 0.01)

    print(f"\n[3/4] Generating control sequences (K={K_adapt})...")

    # Time array
    T = config.get('horizon', 1.0)
    n_segments = config.get('n_segments', 100)
    time = np.linspace(0, T, n_segments)

    # Scenario (a): Robust baseline
    print("\n  (a) Robust baseline...")
    controls_robust, fid_robust = get_control_sequence(
        robust_policy, baseline_task, config, env,
        adapt=False, K=0
    )

    # Scenario (b): Detuning-adapted
    print("  (b) Detuning-adapted...")
    controls_high_det, fid_high_det = get_control_sequence(
        meta_policy, high_detuning_task, config, env,
        adapt=True, K=K_adapt, inner_lr=inner_lr
    )
    controls_low_det, fid_low_det = get_control_sequence(
        meta_policy, low_detuning_task, config, env,
        adapt=True, K=K_adapt, inner_lr=inner_lr
    )

    # Scenario (c): T1-adapted
    print("  (c) T1-adapted (via noise amplitude)...")
    controls_high_noise, fid_high_noise = get_control_sequence(
        meta_policy, high_noise_task, config, env,
        adapt=True, K=K_adapt, inner_lr=inner_lr
    )
    controls_low_noise, fid_low_noise = get_control_sequence(
        meta_policy, low_noise_task, config, env,
        adapt=True, K=K_adapt, inner_lr=inner_lr
    )

    # Plot
    print("\n[4/4] Generating plots...")
    sns.set_style("whitegrid")
    fig, axes = plt.subplots(3, 2, figsize=(16, 12))

    n_controls = config.get('n_controls', 2)
    control_labels = ['X Control', 'Y Control'] if n_controls == 2 else [f'Control {i}' for i in range(n_controls)]
    colors = ['steelblue', 'orangered']

    # (a) Robust baseline
    for ctrl_idx in range(n_controls):
        axes[0, ctrl_idx].plot(time, controls_robust[:, ctrl_idx], linewidth=2,
                               color=colors[ctrl_idx], label='Robust Baseline')
        axes[0, ctrl_idx].set_xlabel('Time (a.u.)', fontsize=12, fontweight='bold')
        axes[0, ctrl_idx].set_ylabel('Control Amplitude', fontsize=12, fontweight='bold')
        axes[0, ctrl_idx].set_title(f'(a) Robust Baseline - {control_labels[ctrl_idx]}\n'
                                    f'Fidelity: {fid_robust:.4f}',
                                    fontsize=13, fontweight='bold')
        axes[0, ctrl_idx].grid(True, alpha=0.3)
        axes[0, ctrl_idx].legend(fontsize=11)

    # (b) Detuning-adapted
    for ctrl_idx in range(n_controls):
        axes[1, ctrl_idx].plot(time, controls_high_det[:, ctrl_idx], linewidth=2,
                               color='purple', label=f'High ωc (ωc={high_detuning_task.omega_c:.1f})',
                               alpha=0.8)
        axes[1, ctrl_idx].plot(time, controls_low_det[:, ctrl_idx], linewidth=2,
                               color='green', label=f'Low ωc (ωc={low_detuning_task.omega_c:.1f})',
                               alpha=0.8, linestyle='--')
        axes[1, ctrl_idx].set_xlabel('Time (a.u.)', fontsize=12, fontweight='bold')
        axes[1, ctrl_idx].set_ylabel('Control Amplitude', fontsize=12, fontweight='bold')
        axes[1, ctrl_idx].set_title(f'(b) Detuning-Adapted - {control_labels[ctrl_idx]}\n'
                                    f'High: {fid_high_det:.4f}, Low: {fid_low_det:.4f}',
                                    fontsize=13, fontweight='bold')
        axes[1, ctrl_idx].grid(True, alpha=0.3)
        axes[1, ctrl_idx].legend(fontsize=10)

    # (c) T1-adapted
    for ctrl_idx in range(n_controls):
        axes[2, ctrl_idx].plot(time, controls_high_noise[:, ctrl_idx], linewidth=2,
                               color='red', label=f'High Noise (A={high_noise_task.A:.2f})',
                               alpha=0.8)
        axes[2, ctrl_idx].plot(time, controls_low_noise[:, ctrl_idx], linewidth=2,
                               color='blue', label=f'Low Noise (A={low_noise_task.A:.2f})',
                               alpha=0.8, linestyle='--')
        axes[2, ctrl_idx].set_xlabel('Time (a.u.)', fontsize=12, fontweight='bold')
        axes[2, ctrl_idx].set_ylabel('Control Amplitude', fontsize=12, fontweight='bold')
        axes[2, ctrl_idx].set_title(f'(c) T1-Adapted - {control_labels[ctrl_idx]}\n'
                                    f'High Noise: {fid_high_noise:.4f}, Low Noise: {fid_low_noise:.4f}',
                                    fontsize=13, fontweight='bold')
        axes[2, ctrl_idx].grid(True, alpha=0.3)
        axes[2, ctrl_idx].legend(fontsize=10)

    plt.suptitle('Control Sequences: Robust vs Adapted Policies',
                 fontsize=16, fontweight='bold', y=0.995)
    plt.tight_layout()

    output_path = f"{output_dir}/control_scenarios.pdf"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nFigure saved to {output_path}")

    output_path_png = f"{output_dir}/control_scenarios.png"
    plt.savefig(output_path_png, dpi=300, bbox_inches='tight')
    print(f"Figure saved to {output_path_png}")

    plt.close()

    # Save numerical data
    results = {
        'robust_baseline': {
            'controls': controls_robust.tolist(),
            'fidelity': float(fid_robust),
            'task': {'alpha': baseline_task.alpha, 'A': baseline_task.A, 'omega_c': baseline_task.omega_c}
        },
        'detuning_adapted': {
            'high': {
                'controls': controls_high_det.tolist(),
                'fidelity': float(fid_high_det),
                'task': {'alpha': high_detuning_task.alpha, 'A': high_detuning_task.A, 'omega_c': high_detuning_task.omega_c}
            },
            'low': {
                'controls': controls_low_det.tolist(),
                'fidelity': float(fid_low_det),
                'task': {'alpha': low_detuning_task.alpha, 'A': low_detuning_task.A, 'omega_c': low_detuning_task.omega_c}
            }
        },
        'T1_adapted': {
            'high_noise': {
                'controls': controls_high_noise.tolist(),
                'fidelity': float(fid_high_noise),
                'task': {'alpha': high_noise_task.alpha, 'A': high_noise_task.A, 'omega_c': high_noise_task.omega_c}
            },
            'low_noise': {
                'controls': controls_low_noise.tolist(),
                'fidelity': float(fid_low_noise),
                'task': {'alpha': low_noise_task.alpha, 'A': low_noise_task.A, 'omega_c': low_noise_task.omega_c}
            }
        }
    }

    results_path = f"{output_dir}/control_scenarios_data.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"Data saved to {results_path}")


def visualize_pulse_evolution(
    meta_policy_path: str,
    robust_policy_path: str,
    config: Dict,
    output_dir: str = "results/control_sequences"
):
    """
    Scenario 4: Pulse evolution over adaptation steps.

    Shows how control pulses change as the policy adapts to a single task
    over K = [0, 1, 3, 5] gradient steps.
    """
    print("\n" + "=" * 80)
    print("CONTROL SEQUENCE VISUALIZATION: Pulse Evolution")
    print("=" * 80)

    os.makedirs(output_dir, exist_ok=True)

    # Load policies
    print("\n[1/4] Loading policies...")
    meta_policy = load_policy_from_checkpoint(
        meta_policy_path, config, eval_mode=False, verbose=False
    )
    robust_policy = load_policy_from_checkpoint(
        robust_policy_path, config, eval_mode=True, verbose=False
    )

    # Create environment
    print("\n[2/4] Creating environment...")
    target_state = get_target_state_from_config(config)
    env = create_quantum_environment(config, target_state)

    # Choose a challenging task (e.g., at edge of training distribution)
    alpha_range = config.get('alpha_range', [0.5, 2.0])
    A_range = config.get('A_range', [0.05, 0.3])
    omega_c_range = config.get('omega_c_range', [2.0, 8.0])

    test_task = NoiseParameters(
        alpha=alpha_range[1],  # High spectral exponent
        A=A_range[1],  # High noise
        omega_c=omega_c_range[0]  # Low cutoff frequency
    )

    K_values = [0, 1, 3, 5]
    inner_lr = config.get('inner_lr', 0.01)

    print(f"\n[3/4] Generating pulse evolution (K = {K_values})...")
    print(f"  Test task: α={test_task.alpha:.2f}, A={test_task.A:.2f}, ωc={test_task.omega_c:.2f}")

    # Time array
    T = config.get('horizon', 1.0)
    n_segments = config.get('n_segments', 100)
    time = np.linspace(0, T, n_segments)

    # Get control sequences for each K
    controls_by_K = {}
    fidelities_by_K = {}

    for K in K_values:
        print(f"  K = {K}...", end=' ')
        controls, fidelity = get_control_sequence(
            meta_policy, test_task, config, env,
            adapt=True, K=K, inner_lr=inner_lr
        )
        controls_by_K[K] = controls
        fidelities_by_K[K] = fidelity
        print(f"Fidelity: {fidelity:.4f}")

    # Also get robust baseline for comparison
    controls_robust, fid_robust = get_control_sequence(
        robust_policy, test_task, config, env,
        adapt=False, K=0
    )

    # Plot
    print("\n[4/4] Generating plots...")
    sns.set_style("whitegrid")
    n_controls = config.get('n_controls', 2)
    fig, axes = plt.subplots(n_controls, 1, figsize=(14, 5*n_controls))

    if n_controls == 1:
        axes = [axes]

    control_labels = ['X Control', 'Y Control'] if n_controls == 2 else [f'Control {i}' for i in range(n_controls)]
    colors = plt.cm.plasma(np.linspace(0, 0.9, len(K_values)))

    for ctrl_idx in range(n_controls):
        ax = axes[ctrl_idx]

        # Plot robust baseline
        ax.plot(time, controls_robust[:, ctrl_idx], linewidth=2.5,
               color='gray', label=f'Robust (F={fid_robust:.3f})',
               alpha=0.5, linestyle=':')

        # Plot evolution over K
        for i, K in enumerate(K_values):
            controls = controls_by_K[K]
            fidelity = fidelities_by_K[K]
            ax.plot(time, controls[:, ctrl_idx], linewidth=2.5,
                   color=colors[i], label=f'K={K} (F={fidelity:.3f})',
                   alpha=0.9)

        ax.set_xlabel('Time (a.u.)', fontsize=13, fontweight='bold')
        ax.set_ylabel('Control Amplitude', fontsize=13, fontweight='bold')
        ax.set_title(f'{control_labels[ctrl_idx]} Evolution Over Adaptation Steps',
                    fontsize=14, fontweight='bold')
        ax.legend(fontsize=11, loc='best', ncol=2)
        ax.grid(True, alpha=0.3)
        ax.tick_params(labelsize=11)

    plt.suptitle(f'Pulse Evolution: Meta-Policy Adaptation\n'
                 f'Task: α={test_task.alpha:.2f}, A={test_task.A:.2f}, ωc={test_task.omega_c:.2f}',
                 fontsize=16, fontweight='bold', y=0.995)
    plt.tight_layout()

    output_path = f"{output_dir}/pulse_evolution.pdf"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nFigure saved to {output_path}")

    output_path_png = f"{output_dir}/pulse_evolution.png"
    plt.savefig(output_path_png, dpi=300, bbox_inches='tight')
    print(f"Figure saved to {output_path_png}")

    plt.close()

    # Save numerical data
    results = {
        'test_task': {'alpha': test_task.alpha, 'A': test_task.A, 'omega_c': test_task.omega_c},
        'robust_baseline': {
            'controls': controls_robust.tolist(),
            'fidelity': float(fid_robust)
        },
        'evolution': {
            str(K): {
                'controls': controls_by_K[K].tolist(),
                'fidelity': float(fidelities_by_K[K])
            }
            for K in K_values
        }
    }

    results_path = f"{output_dir}/pulse_evolution_data.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"Data saved to {results_path}")


@app.command()
def main(
    meta_path: Path = Path("experiments/train_scripts/checkpoints/maml_best_policy.pt"),
    robust_path: Path = Path("experiments/train_scripts/checkpoints/robust_best_policy.pt"),
    output_dir: Path = Path("results/control_sequences"),
    scenario: str = "all"
):
    """
    Visualize control sequences under different adaptation scenarios.

    Args:
        meta_path: Path to trained meta policy checkpoint
        robust_path: Path to robust baseline policy checkpoint
        output_dir: Directory to save results and figures
        scenario: Which scenario to run ('comparison', 'evolution', or 'all')
    """
    # Configuration matching training setup
    config = {
        'num_qubits': 1,
        'n_controls': 2,
        'n_segments': 100,
        'horizon': 1.0,
        'target_gate': 'pauli_x',
        'hidden_dim': 256,
        'n_hidden_layers': 2,
        'inner_lr': 0.01,
        'alpha_range': [0.5, 2.0],
        'A_range': [0.05, 0.3],
        'omega_c_range': [2.0, 8.0],
        'noise_frequencies': [1.0, 5.0, 10.0]
    }

    # Check if policy paths exist
    if not meta_path.exists():
        raise FileNotFoundError(f"Meta policy not found: {meta_path}")
    if not robust_path.exists():
        raise FileNotFoundError(f"Robust policy not found: {robust_path}")

    print(f"Using meta policy: {meta_path}")
    print(f"Using robust policy: {robust_path}")

    # Run requested scenarios
    if scenario in ["comparison", "all"]:
        visualize_scenario_comparison(
            meta_policy_path=str(meta_path),
            robust_policy_path=str(robust_path),
            config=config,
            output_dir=str(output_dir)
        )

    if scenario in ["evolution", "all"]:
        visualize_pulse_evolution(
            meta_policy_path=str(meta_path),
            robust_policy_path=str(robust_path),
            config=config,
            output_dir=str(output_dir)
        )

    print("\n" + "=" * 80)
    print("VISUALIZATION COMPLETE")
    print("=" * 80)


if __name__ == "__main__":
    app()
